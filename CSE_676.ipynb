{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMYYqC61_YIV"
      },
      "outputs": [],
      "source": [
        "# install Segmentation_Models_3D package \n",
        "! pip install segmentation-models-3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import glob\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from tifffile import imsave\n",
        "import os, gzip, shutil\n",
        "import tensorflow\n",
        "import keras\n",
        "import random\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess Dataset\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "LtL-QnUjj3yR",
        "outputId": "4f1be516-bf3a-4df1-8ed5-369c70baed44"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Convert .nii.gz files to .nii\n",
        "    Args:\n",
        "        directory: directory of the downloaded dataset\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def gz_extract(directory):\n",
        "    extension = \".gz\"\n",
        "    os.chdir(directory)\n",
        "    for i in os.listdir(directory): # loop through items in dir\n",
        "        if not os.path.isdir(i):\n",
        "            continue\n",
        "        for item in os.listdir(i):\n",
        "            if item.endswith(extension): # check for \".gz\" extension\n",
        "                gz_name = os.path.abspath(item) # get full path of files\n",
        "                file_name = (os.path.basename(gz_name)).rsplit('.',1)[0] #get file name for file within\n",
        "                gz_name = directory + \"/\" + i + \"/\" + item\n",
        "                file_name = directory + \"/\" + i + \"/\" + item[:-3]\n",
        "                with gzip.open(gz_name,\"rb\") as f_in, open(file_name,\"wb\") as f_out:\n",
        "                    shutil.copyfileobj(f_in, f_out)\n",
        "                os.remove(gz_name) # delete zipped file\n",
        "        \n",
        "gz_extract(\"/content/drive/MyDrive/CSE 676/MICCAI_BraTS2020_TrainingData\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Convert .nii files to .npy\n",
        "\"\"\"\n",
        "\n",
        "t2_list = sorted(glob.glob('/content/drive/MyDrive/CSE 676/MICCAI_BraTS2020_TrainingData/*/*t2.nii'))\n",
        "t1ce_list = sorted(glob.glob('/content/drive/MyDrive/CSE 676/MICCAI_BraTS2020_TrainingData/*/*t1ce.nii'))\n",
        "flair_list = sorted(glob.glob('/content/drive/MyDrive/CSE 676/MICCAI_BraTS2020_TrainingData/*/*flair.nii'))\n",
        "mask_list = sorted(glob.glob('/content/drive/MyDrive/CSE 676/MICCAI_BraTS2020_TrainingData/*/*seg.nii'))\n",
        "\n",
        "\n",
        "for img in range(len(t2_list)):   #Using t1_list as all lists are of same size\n",
        "    print(\"Now preparing image and masks number: \", img)\n",
        "      \n",
        "    temp_image_t2=nib.load(t2_list[img]).get_fdata()\n",
        "    temp_image_t2=scaler.fit_transform(temp_image_t2.reshape(-1, temp_image_t2.shape[-1])).reshape(temp_image_t2.shape)\n",
        "   \n",
        "    temp_image_t1ce=nib.load(t1ce_list[img]).get_fdata()\n",
        "    temp_image_t1ce=scaler.fit_transform(temp_image_t1ce.reshape(-1, temp_image_t1ce.shape[-1])).reshape(temp_image_t1ce.shape)\n",
        "   \n",
        "    temp_image_flair=nib.load(flair_list[img]).get_fdata()\n",
        "    temp_image_flair=scaler.fit_transform(temp_image_flair.reshape(-1, temp_image_flair.shape[-1])).reshape(temp_image_flair.shape)\n",
        "        \n",
        "    temp_mask=nib.load(mask_list[img]).get_fdata()\n",
        "    temp_mask=temp_mask.astype(np.uint8)\n",
        "    temp_mask[temp_mask==4] = 3  #Reassign mask values 4 to 3    \n",
        "    \n",
        "    temp_combined_images = np.stack([temp_image_flair, temp_image_t1ce, temp_image_t2], axis=3)\n",
        "    \n",
        "    #Crop to a size to be divisible by 64 so we can later extract 64x64x64 patches. \n",
        "    #cropping x, y, and z\n",
        "    temp_combined_images=temp_combined_images[56:184, 56:184, 13:141]\n",
        "    temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
        "    \n",
        "    val, counts = np.unique(temp_mask, return_counts=True)\n",
        "    \n",
        "    if (1 - (counts[0]/counts.sum())) > 0.01:  #At least 1% useful volume with labels that are not 0\n",
        "        print(\"Save Me\")\n",
        "        temp_mask= to_categorical(temp_mask, num_classes=4)\n",
        "        np.save('BraTS2020_TrainingData/input_data_3channels/images/image_'+str(img)+'.npy', temp_combined_images)\n",
        "        np.save('BraTS2020_TrainingData/input_data_3channels/masks/mask_'+str(img)+'.npy', temp_mask)\n",
        "        \n",
        "    else:\n",
        "        print(\"I am useless\")   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bi4gbn7qeXB5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Change dataset directory as follows:\n",
        "    root_dir\n",
        "    ├── data\n",
        "    │   ├── train\n",
        "    │   │   ├── images\n",
        "    │   │   ├── masks\n",
        "    │   ├── val\n",
        "    │   │   ├── images\n",
        "    │   │   ├── masks\n",
        "    │   ├── test\n",
        "    │   │   ├── images\n",
        "    │   │   ├── masks\n",
        "    ├── ...\n",
        "\n",
        "    images in train folder:  276\n",
        "    images in val folder:     34\n",
        "    images in test folder:    35\n",
        "    Total:                   345 images after running above cell\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS5WcIs1fULk"
      },
      "source": [
        "## Generating Dataset Generator\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Mq9AlxWiS1d4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    dir: directory of your dataset\n",
        "\"\"\"\n",
        "\n",
        "dir = '/data/'\n",
        "train_img_dir = dir + 'train/images/'\n",
        "train_mask_dir = dir + 'train/masks/'\n",
        "val_img_dir = dir + 'val/images/'\n",
        "val_mask_dir = dir + 'val/masks/'\n",
        "test_img_dir = dir + 'test/images/'\n",
        "test_mask_dir = dir + 'test/masks/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ga_gTRUp-sVr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Custom data generator to work with BraTS2020 dataset.\n",
        "    Can be used as a template to create your own custom data generators. \n",
        "    No image processing operations are performed here, just load data from local directory\n",
        "    in batches. \n",
        "\"\"\"\n",
        "\n",
        "def load_img(img_dir, img_list):\n",
        "    images=[]\n",
        "    for i, image_name in enumerate(img_list):    \n",
        "        if (image_name.split('.')[1] == 'npy'):\n",
        "            \n",
        "            image = np.load(img_dir+image_name)\n",
        "                      \n",
        "            images.append(image)\n",
        "    images = np.array(images)\n",
        "    \n",
        "    return(images)\n",
        "\n",
        "\n",
        "def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size):\n",
        "\n",
        "    L = len(img_list)\n",
        "\n",
        "    #keras needs the generator infinite, so we will use while true  \n",
        "    while True:\n",
        "\n",
        "        batch_start = 0\n",
        "        batch_end = batch_size\n",
        "\n",
        "        while batch_start < L:\n",
        "            limit = min(batch_end, L)\n",
        "                       \n",
        "            X = load_img(img_dir, img_list[batch_start:limit])\n",
        "            Y = load_img(mask_dir, mask_list[batch_start:limit])\n",
        "\n",
        "            for slice_ in range((X.shape[2])):\n",
        "                yield (X[:, :, slice_, :], Y[:, :, slice_, :]) #a tuple with two numpy arrays with batch_size samples     \n",
        "\n",
        "            batch_start += batch_size   \n",
        "            batch_end += batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "VwGHyeCd_Hhy",
        "outputId": "1d2673d3-34ad-4cae-d96c-a6aa5d325b4c"
      },
      "outputs": [],
      "source": [
        "train_img_list=os.listdir(train_img_dir)\n",
        "train_mask_list = os.listdir(train_mask_dir)\n",
        "\n",
        "val_img_list=os.listdir(val_img_dir)\n",
        "val_mask_list = os.listdir(val_mask_dir)\n",
        "\n",
        "print(f\"train list: {len(train_img_list)}\")\n",
        "print(f\"val list: {len(val_img_list)}\")\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_img_datagen = imageLoader(train_img_dir, train_img_list, \n",
        "                                train_mask_dir, train_mask_list, batch_size)\n",
        "\n",
        "val_img_datagen = imageLoader(val_img_dir, val_img_list, \n",
        "                                val_mask_dir, val_mask_list, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Model and Train\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "At15DWCZADNr"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "\n",
        "from layer_utils import *\n",
        "from activations import GELU, Snake\n",
        "from _model_unet_2d import UNET_left, UNET_right\n",
        "from transformer_layers import patch_extract, patch_embedding\n",
        "from _backbone_zoo import backbone_zoo, bach_norm_checker\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, MultiHeadAttention, LayerNormalization, Dense, Embedding\n",
        "    \n",
        "def ViT_MLP(X, filter_num, activation='GELU', name='MLP'):\n",
        "    '''\n",
        "    The MLP block of ViT.\n",
        "    \n",
        "    ----------\n",
        "    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, \n",
        "    T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. \n",
        "    An image is worth 16x16 words: Transformers for image recognition at scale. \n",
        "    arXiv preprint arXiv:2010.11929.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: the input tensor of MLP, i.e., after MSA and skip connections\n",
        "        filter_num: a list that defines the number of nodes for each MLP layer.\n",
        "                        For the last MLP layer, its number of node must equal to the dimension of key.\n",
        "        activation: activation of MLP nodes.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        V: output tensor.\n",
        "\n",
        "    '''\n",
        "    activation_func = eval(activation)\n",
        "    \n",
        "    for i, f in enumerate(filter_num):\n",
        "        X = Dense(f, name='{}_dense_{}'.format(name, i))(X)\n",
        "        X = activation_func(name='{}_activation_{}'.format(name, i))(X)\n",
        "        \n",
        "    return X\n",
        "    \n",
        "def ViT_block(V, num_heads, key_dim, filter_num_MLP, activation='GELU', name='ViT'):\n",
        "    '''\n",
        "    \n",
        "    Vision transformer (ViT) block.\n",
        "    \n",
        "    ViT_block(V, num_heads, key_dim, filter_num_MLP, activation='GELU', name='ViT')\n",
        "    \n",
        "    ----------\n",
        "    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, \n",
        "    T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J., 2020. \n",
        "    An image is worth 16x16 words: Transformers for image recognition at scale. \n",
        "    arXiv preprint arXiv:2010.11929.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        V: embedded input features.\n",
        "        num_heads: number of attention heads.\n",
        "        key_dim: dimension of the attention key (equals to the embeded dimensions).\n",
        "        filter_num_MLP: a list that defines the number of nodes for each MLP layer.\n",
        "                        For the last MLP layer, its number of node must equal to the dimension of key.\n",
        "        activation: activation of MLP nodes.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        V: output tensor.\n",
        "    \n",
        "    '''\n",
        "    # Multiheaded self-attention (MSA)\n",
        "    V_atten = V # <--- skip\n",
        "    V_atten = LayerNormalization(name='{}_layer_norm_1'.format(name))(V_atten)\n",
        "    V_atten = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, \n",
        "                                 name='{}_atten'.format(name))(V_atten, V_atten)\n",
        "    # Skip connection\n",
        "    V_add = add([V_atten, V], name='{}_skip_1'.format(name)) # <--- skip\n",
        "    \n",
        "    # MLP\n",
        "    V_MLP = V_add # <--- skip\n",
        "    V_MLP = LayerNormalization(name='{}_layer_norm_2'.format(name))(V_MLP)\n",
        "    V_MLP = ViT_MLP(V_MLP, filter_num_MLP, activation, name='{}_mlp'.format(name))\n",
        "    # Skip connection\n",
        "    V_out = add([V_MLP, V_add], name='{}_skip_2'.format(name)) # <--- skip\n",
        "    \n",
        "    return V_out\n",
        "\n",
        "\n",
        "def transunet_2d_base(input_tensor, filter_num, stack_num_down=2, stack_num_up=2, \n",
        "                      embed_dim=768, num_mlp=3072, num_heads=12, num_transformer=12,\n",
        "                      activation='ReLU', mlp_activation='GELU', batch_norm=False, pool=True, unpool=True, \n",
        "                      backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='transunet'):\n",
        "    '''\n",
        "    The base of transUNET with an optional ImageNet-trained backbone.\n",
        "    \n",
        "    ----------\n",
        "    Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L. and Zhou, Y., 2021. \n",
        "    Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_tensor: the input tensor of the base, e.g., `keras.layers.Inpyt((None, None, 3))`.\n",
        "        filter_num: a list that defines the number of filters for each \\\n",
        "                    down- and upsampling levels. e.g., `[64, 128, 256, 512]`.\n",
        "                    The depth is expected as `len(filter_num)`.\n",
        "        stack_num_down: number of convolutional layers per downsampling level/block. \n",
        "        stack_num_up: number of convolutional layers (after concatenation) per upsampling level/block.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.\n",
        "        name: prefix of the created keras model and its layers.\n",
        "        \n",
        "        ---------- (keywords of ViT) ----------\n",
        "        embed_dim: number of embedded dimensions.\n",
        "        num_mlp: number of MLP nodes.\n",
        "        num_heads: number of attention heads.\n",
        "        num_transformer: number of stacked ViTs.\n",
        "        mlp_activation: activation of MLP nodes.\n",
        "        \n",
        "        ---------- (keywords of backbone options) ----------\n",
        "        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n",
        "                       None (default) means no backbone. \n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0-7]\n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        freeze_backbone: True for a frozen backbone.\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    '''\n",
        "    activation_func = eval(activation)\n",
        "    \n",
        "    X_skip = []\n",
        "    depth_ = len(filter_num)\n",
        "    \n",
        "    # ----- internal parameters ----- #\n",
        "    \n",
        "    # patch size (fixed to 1-by-1)\n",
        "    patch_size = 1\n",
        "    \n",
        "    # input tensor size\n",
        "    input_size = input_tensor.shape[1]\n",
        "    \n",
        "    # encoded feature map size\n",
        "    encode_size = input_size // 2**(depth_-1)\n",
        "    \n",
        "    # number of size-1 patches\n",
        "    num_patches = encode_size ** 2 \n",
        "    \n",
        "    # dimension of the attention key (= dimension of embedings)\n",
        "    key_dim = embed_dim\n",
        "    \n",
        "    # number of MLP nodes\n",
        "    filter_num_MLP = [num_mlp, embed_dim]\n",
        "    \n",
        "    # ----- UNet-like downsampling ----- #\n",
        "    \n",
        "    # no backbone cases\n",
        "    if backbone is None:\n",
        "\n",
        "        X = input_tensor\n",
        "\n",
        "        # stacked conv2d before downsampling\n",
        "        X = CONV_stack(X, filter_num[0], stack_num=stack_num_down, activation=activation, \n",
        "                       batch_norm=batch_norm, name='{}_down0'.format(name))\n",
        "        X_skip.append(X)\n",
        "\n",
        "        # downsampling blocks\n",
        "        for i, f in enumerate(filter_num[1:]):\n",
        "            X = UNET_left(X, f, stack_num=stack_num_down, activation=activation, pool=pool, \n",
        "                          batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))        \n",
        "            X_skip.append(X)\n",
        "\n",
        "    # backbone cases\n",
        "    else:\n",
        "        # handling VGG16 and VGG19 separately\n",
        "        if 'VGG' in backbone:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_skip = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_skip)\n",
        "            \n",
        "        # for other backbones\n",
        "        else:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_-1, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_skip = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_skip) + 1\n",
        "\n",
        "\n",
        "        # extra conv2d blocks are applied\n",
        "        # if downsampling levels of a backbone < user-specified downsampling levels\n",
        "        if depth_encode < depth_:\n",
        "\n",
        "            # begins at the deepest available tensor  \n",
        "            X = X_skip[-1]\n",
        "\n",
        "            # extra downsamplings\n",
        "            for i in range(depth_-depth_encode):\n",
        "                i_real = i + depth_encode\n",
        "\n",
        "                X = UNET_left(X, filter_num[i_real], stack_num=stack_num_down, activation=activation, pool=pool, \n",
        "                              batch_norm=batch_norm, name='{}_down{}'.format(name, i_real+1))\n",
        "                X_skip.append(X)\n",
        "        \n",
        "    # subtrack the last tensor (will be replaced by the ViT output)\n",
        "    X = X_skip[-1]\n",
        "    X_skip = X_skip[:-1]\n",
        "\n",
        "    # 1-by-1 linear transformation before entering ViT blocks\n",
        "    X = Conv2D(filter_num[-1], 1, padding='valid', use_bias=False, name='{}_conv_trans_before'.format(name))(X)\n",
        "\n",
        "    X = patch_extract((patch_size, patch_size))(X)\n",
        "    X = patch_embedding(num_patches, embed_dim)(X)\n",
        "\n",
        "    # stacked ViTs \n",
        "    for i in range(num_transformer):\n",
        "        X = ViT_block(X, num_heads, key_dim, filter_num_MLP, activation=mlp_activation, \n",
        "                      name='{}_ViT_{}'.format(name, i))\n",
        "\n",
        "    # reshape patches to feature maps\n",
        "    X = tf.reshape(X, (-1, encode_size, encode_size, embed_dim))\n",
        "\n",
        "    # 1-by-1 linear transformation to adjust the number of channels\n",
        "    X = Conv2D(filter_num[-1], 1, padding='valid', use_bias=False, name='{}_conv_trans_after'.format(name))(X)\n",
        "\n",
        "    X_skip.append(X)\n",
        "    \n",
        "    # ----- UNet-like upsampling ----- #\n",
        "    \n",
        "    # reverse indexing encoded feature maps\n",
        "    X_skip = X_skip[::-1]\n",
        "    # upsampling begins at the deepest available tensor\n",
        "    X = X_skip[0]\n",
        "    # other tensors are preserved for concatenation\n",
        "    X_decode = X_skip[1:]\n",
        "    depth_decode = len(X_decode)\n",
        "\n",
        "    # reverse indexing filter numbers\n",
        "    filter_num_decode = filter_num[:-1][::-1]\n",
        "\n",
        "    # upsampling with concatenation\n",
        "    for i in range(depth_decode):\n",
        "        X = UNET_right(X, [X_decode[i],], filter_num_decode[i], stack_num=stack_num_up, activation=activation, \n",
        "                       unpool=unpool, batch_norm=batch_norm, name='{}_up{}'.format(name, i))\n",
        "\n",
        "    # if tensors for concatenation is not enough\n",
        "    # then use upsampling without concatenation \n",
        "    if depth_decode < depth_-1:\n",
        "        for i in range(depth_-depth_decode-1):\n",
        "            i_real = i + depth_decode\n",
        "            X = UNET_right(X, None, filter_num_decode[i_real], stack_num=stack_num_up, activation=activation, \n",
        "                       unpool=unpool, batch_norm=batch_norm, concat=False, name='{}_up{}'.format(name, i_real))\n",
        "            \n",
        "    return X\n",
        "\n",
        "def transunet_2d(input_size, filter_num, n_labels, stack_num_down=2, stack_num_up=2,\n",
        "                 embed_dim=768, num_mlp = 3072, num_heads=12, num_transformer=12,\n",
        "                 activation='ReLU', mlp_activation='GELU', output_activation='Softmax', batch_norm=False, pool=True, unpool=True, \n",
        "                 backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='transunet'):\n",
        "    '''\n",
        "    TransUNET with an optional ImageNet-trained bakcbone.\n",
        "    \n",
        "    \n",
        "    ----------\n",
        "    Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L. and Zhou, Y., 2021. \n",
        "    Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_size: the size/shape of network input, e.g., `(128, 128, 3)`.\n",
        "        filter_num: a list that defines the number of filters for each \\\n",
        "                    down- and upsampling levels. e.g., `[64, 128, 256, 512]`.\n",
        "                    The depth is expected as `len(filter_num)`.\n",
        "        n_labels: number of output labels.\n",
        "        stack_num_down: number of convolutional layers per downsampling level/block. \n",
        "        stack_num_up: number of convolutional layers (after concatenation) per upsampling level/block.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        output_activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n",
        "                           Default option is 'Softmax'.\n",
        "                           if None is received, then linear activation is applied.\n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.                 \n",
        "        name: prefix of the created keras model and its layers.\n",
        "        \n",
        "        ---------- (keywords of ViT) ----------\n",
        "        embed_dim: number of embedded dimensions.\n",
        "        num_mlp: number of MLP nodes.\n",
        "        num_heads: number of attention heads.\n",
        "        num_transformer: number of stacked ViTs.\n",
        "        mlp_activation: activation of MLP nodes.\n",
        "        \n",
        "        ---------- (keywords of backbone options) ----------\n",
        "        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n",
        "                       None (default) means no backbone. \n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0-7]\n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        freeze_backbone: True for a frozen backbone.\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        model: a keras model.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "        \n",
        "    IN = Input(input_size)\n",
        "    \n",
        "    # base    \n",
        "    X = transunet_2d_base(IN, filter_num, stack_num_down=stack_num_down, stack_num_up=stack_num_up, \n",
        "                          embed_dim=embed_dim, num_mlp=num_mlp, num_heads=num_heads, num_transformer=num_transformer,\n",
        "                          activation=activation, mlp_activation=mlp_activation, batch_norm=batch_norm, pool=pool, unpool=unpool,\n",
        "                          backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, freeze_batch_norm=freeze_batch_norm, name=name)\n",
        "    \n",
        "    # output layer\n",
        "    OUT = CONV_output(X, n_labels, kernel_size=1, activation=output_activation, name='{}_output'.format(name))\n",
        "    \n",
        "    # functional API model\n",
        "    model = Model(inputs=[IN,], outputs=[OUT,], name='{}_model'.format(name))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jX4mHUR9CZku"
      },
      "outputs": [],
      "source": [
        "model_ = transunet_2d((128, 128, 3), filter_num=[64, 128], n_labels=4, stack_num_down=2, stack_num_up=2,\n",
        "                                embed_dim=768, num_mlp=512, num_heads=4, num_transformer=4,\n",
        "                                activation='ReLU', mlp_activation='ReLU', output_activation='Softmax', \n",
        "                                batch_norm=True, pool=True, unpool='bilinear', name='transunet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qkZxfPRN_tgz"
      },
      "outputs": [],
      "source": [
        "#Define loss, metrics and optimizer to be used for training\n",
        "wt0, wt1, wt2, wt3 = 0.25,0.25,0.25,0.25\n",
        "import segmentation_models_3D as sm\n",
        "dice_loss = sm.losses.DiceLoss(class_weights=np.array([wt0, wt1, wt2, wt3])) \n",
        "focal_loss = sm.losses.CategoricalFocalLoss()\n",
        "total_loss = dice_loss + (1 * focal_loss)\n",
        "\n",
        "metrics = ['accuracy', sm.metrics.IOUScore(threshold=0.75)]\n",
        "\n",
        "LR = 0.001\n",
        "optim = tensorflow.keras.optimizers.Adam(LR)\n",
        "\n",
        "steps_per_epoch = len(train_img_list) // 1\n",
        "val_steps_per_epoch = len(val_img_list) // 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pizkzSajDvH1",
        "outputId": "499dcf86-f905-4417-9343-2574edb24d1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 128, 128, 3)\n",
            "(None, 128, 128, 4)\n"
          ]
        }
      ],
      "source": [
        "model_.compile(optimizer = optim, loss=total_loss, metrics=metrics)\n",
        "# print(model_.summary())\n",
        "\n",
        "print(model_.input_shape)\n",
        "print(model_.output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "AN7uHpv_EAh0",
        "outputId": "696b6489-8d12-4989-ba10-d51162b47790"
      },
      "outputs": [],
      "source": [
        "history=model_.fit(train_img_datagen,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          epochs=3,\n",
        "          verbose=1,\n",
        "          validation_data=val_img_datagen,\n",
        "          validation_steps=val_steps_per_epoch,\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "M0QJJ6K6iIHn",
        "outputId": "33cb7160-f129-4c0c-ffaf-b5d7127d418d"
      },
      "outputs": [],
      "source": [
        "# plot training and validation loss\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "-0YiUKXaiKPm",
        "outputId": "3b51e9bb-a00b-41f6-84a4-213aab39747f"
      },
      "outputs": [],
      "source": [
        "# plot training and validation accuracy\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "plt.plot(epochs, acc, 'y', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFussFfPEIKV"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "model_.save('/content/drive/MyDrive/CSE 676/pretrained/brats_3d.hdf5')\n",
        "tensorflow.saved_model.save(model_, \"/content/drive/MyDrive/CSE 676/pretrained/brats\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OdMoSJknb0fM"
      },
      "outputs": [],
      "source": [
        "# Define dice coefficient\n",
        "\n",
        "def single_dice_coef(y_true, y_pred_bin):\n",
        "    # shape of y_true and y_pred_bin: (height, width)\n",
        "    intersection = np.sum(y_true * y_pred_bin)\n",
        "    if (np.sum(y_true)==0) and (np.sum(y_pred_bin)==0):\n",
        "        return 1\n",
        "    return (2*intersection) / (np.sum(y_true) + np.sum(y_pred_bin))\n",
        "\n",
        "def mean_dice_coef(y_true, y_pred_bin):\n",
        "    # shape of y_true and y_pred_bin: (n_samples, height, width, n_channels)\n",
        "    batch_size = y_true.shape[0]\n",
        "    channel_num = y_true.shape[-1]\n",
        "    mean_dice_channel = 0.\n",
        "    for i in range(batch_size):\n",
        "        for j in range(channel_num):\n",
        "            channel_dice = single_dice_coef(y_true[i, :, :, j], y_pred_bin[i, :, :, j])\n",
        "            mean_dice_channel += channel_dice/(channel_num*batch_size)\n",
        "    return mean_dice_channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gwfQPJLoBSy",
        "outputId": "0d05bd95-ee39-4855-cbb9-f004a8454a6c"
      },
      "outputs": [],
      "source": [
        "# Calculate dice coefficient on test data\n",
        "avg_IOU = 0\n",
        "total = 0\n",
        "images = 0\n",
        "\n",
        "test_img_list = os.listdir(test_img_dir)\n",
        "test_mask_list = os.listdir(test_mask_dir)\n",
        "\n",
        "for idx in range(len(test_img_list)):\n",
        "    X = load_img(test_img_dir, test_img_list[idx:idx+1])\n",
        "    Y = load_img(test_mask_dir, test_mask_list[idx:idx+1])\n",
        "\n",
        "    for i in range(128):\n",
        "        img = X[:, :, :, i, :]\n",
        "        mask = Y[:, :, :, i, :]\n",
        "\n",
        "        pred = model_.predict(img)\n",
        "\n",
        "        overlap = mask * pred # Logical AND\n",
        "        union = mask + pred # Logical OR\n",
        "        IOU = overlap.sum()/float(union.sum())\n",
        "        total += 1\n",
        "        avg_IOU += IOU\n",
        "\n",
        "    images += 1\n",
        "    print(f\"completed images: {images}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34T3SHMUeS0k",
        "outputId": "632f6be7-2693-487e-febd-f423590e9d47"
      },
      "outputs": [],
      "source": [
        "print(f\"Final dice coefficient: {avg_IOU / total}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Download the trained models from the following link:\n",
        "    https://drive.google.com/drive/folders/141rN2-R1gAOfyntgU5lP1qFVgaHBRWKY?usp=sharing\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "fS5WcIs1fULk"
      ],
      "machine_shape": "hm",
      "name": "CSE 676.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
